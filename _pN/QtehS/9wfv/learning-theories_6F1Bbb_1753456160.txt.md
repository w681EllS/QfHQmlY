# Bibliography

* Agarwal, A., Jing, N., Kakade, S. M., Sun, W. (2022). Reinforcement Learning: Theory and Algorithms (draft). https://rltheorybook.github.io/
* Amari, S. (1998). Natural gradient works efficiently in learning. https://doi.org/10.1162/089976698300017746
* Auer, P., Cesa-Bianchi, N., Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. https://doi.org/10.1023/A:1013689704352
* Azar, M. G., Osband, I., Munos, R. (2017). Minimax regret bounds for reinforcement learning. https://arxiv.org/abs/1703.05449
* Baird, L. (1995). Residual algorithms: reinforcement learning with function approximation. https://doi.org/10.1016/B978-1-55860-377-6.50013-X
* Barto, A. G., Sutton, R. S., Anderson, C. W. (1983). Neuronlike adaptive elements that can solve difficult learning control problems. https://doi.org/10.1109/TSMC.1983.6313077
* Bellemare, M. G., Dabney, W., Munos, R. (2017). A distributional perspective on reinforcement learning. https://proceedings.mlr.press/v70/bellemare17a.html
* Bellman, R. E. (1957). Dynamic Programming. Princeton University Press.
* Blum, J. R. (1954). Approximation methods which converge with probability one. https://doi.org/10.1214/aoms/1177728794
* Christina, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep reinforcement learning from human preferences. https://arxiv.org/abs/1706.03741
* Dabney, W., Ostrovski, G., Silver, D., Munos, R. (2018). Implicit quantile networks for distributional reinforcement learning. https://arxiv.org/abs/1806.06923
* Dabney, W., Rowland, M., Bellemare, M. G., Munos, R. (2018). Distributional reinforcement learning with quantile regression. https://ojs.aaai.org/index.php/AAAI/article/view/11791
* DeJong, G., Spong, M. W. (1994). Swinging up the Acrobot: an example of intelligent control. https://doi.org/10.1109/ACC.1994.752458
* Dietterich, T. G. (2000). Hierarchical reinforcement learning with the MAXQ value function decomposition. https://doi.org/10.1613/jair.639
* Fujimoto, S., Hoof, H. v., Meger, D. (2018). Addressing function approximation error in actor–critic methods. https://arxiv.org/abs/1802.09477
* Haarnoja, T., Tang, H., Abbeel, P., Levine, S. (2017). Reinforcement learning with deep energy-based policies. https://arxiv.org/abs/1702.08165
* Haarnoja, T., Zhou, A., Abbeel, P., Levine, S. (2018). Soft actor–critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. https://arxiv.org/abs/1801.01290
* Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., ... Leviney, S. (2018). Soft actor-critic algorithms and applications. https://arxiv.org/abs/1812.05905
* Hasselt, H. v. (2010). Double Q-learning. https://dl.acm.org/doi/abs/10.5555/2997046.2997187
* Hasselt, H. v., Guez, A., Silver, D. (2015). Deep reinforcement learning with double Q-learning. https://arxiv.org/abs/1509.06461
* Ho, J., Ermon, S. (2016). Generative adversarial imitation learning. https://arxiv.org/abs/1606.03476
* Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., van Hasselt, H., Silver, D. (2018). Distributed prioritized experience replay. https://arxiv.org/abs/1803.00933
* Kaiming, H., Xiangyu, Z., Shaoqing, R., Jian, S. (2016). Deep residual learning for image recognition. https://doi.org/10.1109/CVPR.2016.90
* Kakade, S. (2002). A natural policy gradient. https://proceedings.neurips.cc/paper/2001/file/4b86abe48d358ecf194c56c69108433e-Paper.pdf
* Kakade, S., Langford, J. (2002). Approximate optimal approximate reinforce learning. ICML.
* Klopf, A. H. (1972). Brain function and adaptive systems: A heterostatic theory. Technical Report AFCRL-72-0164, Air Force Cambridge Research Laboratories.
* Lai, T. L., Robbins, H. (1985). Asymptotically efficient adaptive allocation rules. https://doi.org/10.1016/0196-8858(85)90002-8
* Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... Wierstra, D. (2016). Continuous control with deep reinforcement learning. https://arxiv.org/abs/1509.02971
* Lin, L.-J. (1993). Reinforcement Learning for robots using neural networks. Ph.D. dissertation. Pittsburgh, PA: Carnegie Mellon University.
* Mania, H., Guy, A., Recht, B. (2018). Simple random search provides a competitive approach to reinforcement learning. https://arxiv.org/abs/1803.07055
* McDiarmid, C. (1989). On the method of bounded differences. https://doi.org/10.1017/CBO9781107359949.008
* Michael, J. (1975). Positive and negative reinforcement, a distinction that is no longer necessary; or a better way to talk about bad things. https://www.jstor.org/stable/27758829
* Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... Hassabis, D. (2015). Human-level control through deep reinforcement learning. https://doi.org/10.1038/nature14236
* Moore, A. W. (1990). Efficient Memory-based Learning for Robot Control. Ph.D. dissertation. Cambridge, UK: University of Cambridge.
* Nemirovski, A. S., Yudin, D. B. (1983). Problem complexity and method efficiency in optimization. Wiley.
* Neumann, J. v., Morgenstern, O. (1953). Theory of Games and Economic Behavior. Princeton University Press.
* Ouyang, L., Wu, J., Jing, X., Almeida, D., Wainwright, C. L., ..., Christiano, P., (2022). Training language models to follow instructions with human feedback. https://arxiv.org/abs/2203.02155
* Pavlov, I. P. (1928). Lectures on Conditioned Reflexes, Volume 1 (English translation). International Publishers.
* Robbins, H., Monro, S. (1951). A stochastic approximation algorithm. https://doi.org/10.1214/aoms/1177729586
* Rummery, G. A., Niranjan, M. (1994). On-line Q-learning using connectionist systems. Technical Report CUED/F-INFENG/TR 166, Cambridge University.
* Salimans, T., Ho, J., Chen, X. Sidor, S., Sutskever, I. (2017). Evolution strategies as a scalable alternative to reinforcement learning. https://arxiv.org/abs/1703.03864.
* Samuel, A. L. (1959). Some studies in machine learning using the game of checkers. https://doi.org/10.1147/rd.441.0206
* Schaul, T., Quan, J., Antonoglou, I., Silver, D. (2016). Prioritized experience replay. https://arxiv.org/abs/1511.05952
* Schrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K., Sifre, L., Schmitt, S., ... Silver, D. (2020). Mastering Atari, Go, chess and shogi by planning with a learned model. https://doi.org/10.1038/s41586-020-03051-4
* Schulman, J., Levine, S., Abbeel, P. (2017). Trust region policy optimization. https://arxiv.org/abs/1502.05477
* Schulman, J., Moritz, P., Levine, S., Jordan, M. I., Abbeel, P. (2016). High-dimensional continuous control using generalized advantage estimation. https://arxiv.org/abs/1506.02438
* Schulman, J., Wolski, F., Dhariwal, P., Radford, A., Klimov, O. (2017). Proximal policy optimization algorithms. https://arxiv.org/abs/1707.06347
* Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Driessche, G. v., ... Hassabis, D. (2016). Mastering the game of Go with deep neural networks and tree search. https://doi.org/10.1038/nature16961
* Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., ... Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. https://doi.org/10.1126/science.aar6404
* Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., Riedmiller, M. (2014). Deterministic policy gradient algorithms. https://dl.acm.org/doi/10.5555/3044805.3044850
* Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., ... Hassabis, D. (2017). Mastering the game of Go without human knowledge. https://doi.org/10.1038/nature24270
* Singh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces. https://doi.org/10.1023/A:1018012322525
* Sondik, E. J. (1971). The optimal control of partially observable Markov processes. Ph.D. dissertation. Stanford University.
* Sutton, R. S., Barto, A. G. (1981). Toward a modern theory of adaptive networks: Expectation and prediction. Psychological Review, 88(2):135–170.
* Sutton, R. S., Barto, A. G. (1998). Reinforcement Learning: An introduction. MIT Press.
* Tsitsklis, J. N. (1994). Asynchronous stochastic approximation and Q-learning. https://doi.org/10.1023/A:1022689125041
* Wang, Z., Schaul, T., Hessel, M., Hasselt, H. v., Lanctot, M., Freitas, N. d. (2015). Dueling network architectures for deep reinforcement learning. https://arxiv.org/abs/1511.06581
* Watkins, C. J. (1989). Learning from delayed rewards. Ph.D. dissertation. Cambridge, UK: University of Cambridge.
* Watkins, C. J., Dayan, P. (1992). Q-learning. http://doi.org/10.1007/BF00992698
* Widrow, B., Hoff, M. E. (1960). Adaptive switching circuits. https://doi.org/10.1109/TSMC.1973.4309272
* Widrow, B., Gupta, N. K., Maitra, S. (1973). Punish/reward: Learning with a critic in adaptive threshold systems. https://doi.org/10.1109/TSMC.1973.4309272
* Williams, R. J. (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. https://doi.org/10.1007/BF00992696
* Xiao, Z. (2018). Neural Network and Applications of PyTorch (in Chinese). China Machine Press.
* Xiao, Z. (2019). Reinforcement Learning: Theory and Python Implementation (in Chinese). China Machine Press.
* Yaari, M. E. (1987). The dual theory of choice under risk. https://doi.org/10.2307/1911158
